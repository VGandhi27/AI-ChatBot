'''
************************************************************************************************

    FileName : RagEngine.py
    File Description : File handles ragengine response
    Created By : Vidushi Gandhi
    Date : 9th April 2025

************************************************************************************************
'''

# Import Modules 
import requests

# ğŸ”¹ Optional: Clean up repetitive/robotic phrases
def clean_response(text):
    unwanted = [
        "Based on the information provided",
        "According to the context",
        "Based on the context",
        "According to the information provided",
    ]
    for phrase in unwanted:
        text = text.replace(phrase, "")
    return text.strip().capitalize()

# ğŸ”¹ Generate LLM Response using Ollama phi
def rag_response(context, question, model="phi"):
    prompt =f"""
Youâ€™re an AI assistant designed to help users learn more about Vidushi Gandhiâ€™s work, skills, and accomplishments.

Answer the user's question honestly and clearly using only the information given below. 
Keep your tone friendly and natural â€” like you're having a thoughtful conversation. 
If something isnâ€™t mentioned, donâ€™t guess. Just say:
â€œIâ€™m not sure about that.â€

Avoid robotic phrases like â€œbased on the contextâ€ or â€œaccording to the informationâ€.

---
Hereâ€™s what you know:
Information:
{context}

User's Question: {question}

Answer:
""".strip()

    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={"model": model, "prompt": prompt, "stream": False},
        )

        if response.status_code == 200:
            answer = response.json().get("response", "I'm not sure about that.")
            return clean_response(answer)
        else:
            return f"Error: {response.text}"

    except Exception as e:
        return f"Error connecting to model: {str(e)}"
